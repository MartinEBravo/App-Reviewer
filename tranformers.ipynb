{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ¿Que dicen los LLM de nuestros datos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero importaremos los datos y le aplicaremos la función de rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leemos el csv\n",
    "df = pd.read_csv(\"threads.csv\")\n",
    "df = df.drop([\"source\", \"review_date\"], axis=1)\n",
    "# change the column names to match the HuggingFace dataset format\n",
    "df = df.rename(columns={\"review\": \"text\", \"rating\": \"label\"})\n",
    "\n",
    "# función para convertir el rating a palabras\n",
    "def ratingTransform(rating):\n",
    "    if rating <= 2:\n",
    "        return 0\n",
    "    elif rating <= 4:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "# Transform the ratings to positive, neutral, and negative\n",
    "df[\"label\"] = df[\"label\"].apply(ratingTransform)\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.33)\n",
    "\n",
    "# guardamos los datos en un csv\n",
    "train.to_csv(\"train.csv\", index=False)\n",
    "test.to_csv(\"test.csv\", index=False)\n",
    "\n",
    "# cargamos los datos en un dataset de huggingface\n",
    "dataset = load_dataset(\"csv\", data_files={\"train\": \"train.csv\", \"test\": \"test.csv\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando Tokenizers\n",
    "\n",
    "Los tokenizers son el proceso de convertir una secuencia de texto en una secuencia de tokens (números que hacen referencia a palabras)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenemos un modelo DistilBERT\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Tokenizamos los datos\n",
    "def tokenize_data(example):\n",
    "    return tokenizer(example['review_description'], padding='max_length')\n",
    "\n",
    "# tokenizamos los datos\n",
    "dataset = dataset.map(tokenize_data, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargamos el modelo DistilBERT\n",
    "\n",
    "DistilBERT es un modelo de BERT que es más pequeño y rápido, pero que mantiene una precisión similar a la de BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Cargamos un modelo pre-entrenado\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)\n",
    "\n",
    "batch_size = 8\n",
    "number_of_epochs = 7\n",
    "logging_steps = len(dataset['train']) // batch_size\n",
    "steps = (len(dataset['train']) // batch_size) * number_of_epochs\n",
    "warmup_steps = int(0.2*steps)\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "                                  num_train_epochs=number_of_epochs, \n",
    "                                  load_best_model_at_end=True,\n",
    "                                  evaluation_strategy='steps', \n",
    "                                  save_strategy='steps',\n",
    "                                  learning_rate=2e-5,\n",
    "                                  logging_steps=logging_steps,\n",
    "                                  warmup_steps= warmup_steps,\n",
    "                                  save_steps=1000,\n",
    "                                  eval_steps=500,\n",
    "                                  output_dir=\"fine-tuned-distilbert-base-uncased\"\n",
    "                                  )\n",
    "\n",
    "# shuffle the dataset\n",
    "train_dataset = dataset['train'].shuffle(seed=10) \n",
    "eval_dataset = dataset['test'].shuffle(seed=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning de DistilBERT\n",
    "\n",
    "El fine-tuning es un proceso de entrenamiento en el que se usa un modelo pre-entrenado y se ajusta para que se adapte a los datos específicos del problema. En este caso, usaremos el modelo pre-entrenado de DistilBERT y lo ajustaremos para que se adapte a nuestros datos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "                )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación del modelo\n",
    "\n",
    "Ahora que hemos entrenado nuestro modelo, podemos evaluarlo en el conjunto de datos de prueba, para ver cómo se desempeña en datos que nunca ha visto antes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datasets import load_metric\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    # load the metrics to use\n",
    "    load_accuracy = load_metric(\"accuracy\")\n",
    "    load_f1 = load_metric(\"f1\")\n",
    "\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    # calculate the mertic using the predicted and true value \n",
    "    accuracy = load_accuracy.compute(predictions=predictions, references=labels)\n",
    "    f1 = load_f1.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "    return {\"accuracy\": accuracy, \"f1score\": f1}\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretación del modelo\n",
    "\n",
    "Ahora veremos que palabras son las que más influyen en la predicción del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpretabilidad del modelo\n",
    "\n",
    "from transformers_interpret import SequenceClassificationExplainer\n",
    "\n",
    "# load the explainer\n",
    "explainer = SequenceClassificationExplainer(\n",
    "    model,\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "# explain the first sample in the test set\n",
    "idx = 0\n",
    "explanation = explainer(dataset['test'][idx]['input_ids'])\n",
    "\n",
    "# print the results\n",
    "print(\"The word importance is: \", explanation.word_importances)\n",
    "print(\"The tokens are: \", explanation.words)\n",
    "print(\"The positive and negative contributions are: \", explanation.contributions)\n",
    "print(\"The attribution is: \", explanation.attributions)\n",
    "print(\"The visualization is: \")\n",
    "explanation.visualize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
