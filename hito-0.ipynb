{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hito 1 - AppEval\n",
    "\n",
    "CC5205 - Minería de Datos\n",
    "\n",
    "- Profesora: Jazmine Maldonado\n",
    "- Auxiliar: Fran Zautzik\n",
    "\n",
    "Integrantes:\n",
    "- Felipe Avendaño\n",
    "- Martín Bravo\n",
    "- Franco González\n",
    "- Daniel Radrigrán\n",
    "- Felipe Valdebenito"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introducción\n",
    "\n",
    "En la actualidad, las redes sociales constituyen una de las principales formas de comunicación en el mundo. Algunas de estas redes concentran una cantidad tan grande de personas que las usan, que una alternativa a elas suena como algo impensable. Aún así, algunas corporaciones lanzan su perspectiva de como debiese ser alguna red social en cuestion (séase de ejemplo, la existencia de Telegram frente a WhatsApp como aplicaciones de mensajería instantánea). Dichas redes han sido desarrolladas en base a estudios de mercado, análisis de información, y para este caso, se mencionara el feedback otorgado directamente por los usuarios que conforman la plataforma en sí.\n",
    "\n",
    "Comercialmente hablando, la retroalimentacion de los usuarios sobre una determinada aplicacion es determinante a la hora de realizar mejoras que atraigan y retengan usuarios dentro de la misma plataforma. Tomando esto en cuenta, dada la gran cantidad de comentarios negativos con descripciones vagas o de exagerada negatividad, se complejiza el poder obtener un feedback objetivo que indique precisamente las falencias de la app. Por estas razones, poder extraer determinados fragmentos que se frecuentan en comentarios negativos es de importancia para ayudar a las empresas a mantener la calidad de sus aplicaciones.\n",
    "\n",
    "Los datos que se usarán en el proyecto a presentar, corresponden a reviews de usuarios de aplicaciones de redes sociales, en particular, la aplicación Threads. El estudio de estos datos puede permitir el observar que aspectos de las aplicaciones valoran los usuarios. Tambien permite entender las razones del porque no gusta determinada caracteristica. La elección de datos se basa en la relevancia que poseen para el desarrollo de nuevas redes sociales o mejoras de las ya existentes.\n",
    "\n",
    "Para las emperesas es de particular interés el uso de tecnicas de mineria de datos para obtener los aspectos criticados en las reviews de la app a partir del análisis del lenguaje empleado.\n",
    "\n",
    "Como bien se ha mencionado, estudiar estos datos nos permite entender cuales son los aspectos a mejorar de una app, los cuales no pueden ser conocidos mediante una simple puntuación numérica (e.g. 1-5 estrellas), pues solo indica el nivel de satisfacción que el usuario tiene sobre la app, sin otorgar detalle sobre la razón de dicha calificación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploración de Datos\n",
    "\n",
    "Por medio del uso de Python, junto con variadas librerias para la manipulación del dataset escogido, se procedera a la obtención de tablas y gráficos para extraer información general sobre los comentarios dados a la aplicación Threads. Dicha aplicación se basa en la publicación de mensajes breves a modo de que cada usuario participa en \"foros\" según los tópicos más importantes del día."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías\n",
    "\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy as np # linear algebra\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "import seaborn as sns # plotting\n",
    "import string # string manipulation\n",
    "import re # regular expressions\n",
    "import nltk # text manipulation\n",
    "\n",
    "from tqdm import trange # progress bar\n",
    "from nltk import tokenize # text manipulation\n",
    "from nltk.corpus import stopwords # text manipulation\n",
    "from nltk.stem import WordNetLemmatizer # text manipulation\n",
    "from nltk.probability import FreqDist # text manipulation\n",
    "from collections import Counter # text manipulation\n",
    "from sklearn.feature_extraction.text import CountVectorizer # text manipulation\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator # wordcloud generator\n",
    "from IPython.display import display # image display\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar se mostrará una vista preliminar del dataset escogido. Para ver cantidad de columnas y los tipos de datos que se manejan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"threads.csv\") # carga del dataset a usar como variable\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dicho dataset cuenta con 4 columnas, y 32910 filas. Las columnas son:\n",
    "- Source: La procedencia de los comentarios (según el sistema operativo y la tienda de aplicaciones de la cual se descargó),\n",
    "- Review Description: El comentario publicado, el cual contiene el detalle sobre la calificación otorgada a la aplicación.\n",
    "- Rating: La puntuación otorgada, en una escala de 1 a 5, con 1 como puntuacion mala y 5 como excelente.\n",
    "- Review Date: Fecha de publicación de la reseña, con detalle sobre el día, mes, año y hora de publicación.\n",
    "\n",
    "Se hará una revisión a la cantidad de reseñas segun la calificación otorgada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[\"rating\"].value_counts()) # contar la cantidad de reviews recibidas segun el rating de 1 a 5\n",
    "\n",
    "# muestra mediana, media, desviación estándar, mínimo y máximo de la variable \"replies\"\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que el promedio de los ratings es de 3.4, lo cual es un valor bastante bajo, además podemos observar que la mayoría de reseñas son de 5 estrellas, y luego vienen las de 1 estrella, esto nos sugiere dividir las reseñas en distintos grupos.\n",
    "\n",
    "A partir de estos datos, se puede generar un gráfico de pastel. Primero, se realizará una conversión del rating según rangos. Dichos rangos son: 1-2 estrellas como NEGATIVO, 3-4  estrellas como NEUTRAL y 5 estrellas como POSITIVO. El gráfico generado se muestra a continuación.\n",
    "\n",
    "La razón de esta elección es debido a que los comentarios de 1-2 estrellas suelen ser de tipos negativos, mientras que los de 5 estrellas suelen ser positivos. Los comentarios de 3-4 estrellas los pondremos como neutrales, ya que dicen algo positivo de la aplicación, pero también mencionan algo negativo (en un principio dejamos neutral como 3 estrellas pero esto generaba muy poca cantidad de reseñas neutrales)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# función para convertir el rating a palabras\n",
    "def ratingTransform(rating):\n",
    "    if rating <= 2:\n",
    "        return \"NEGATIVE\"\n",
    "    elif rating <= 4:\n",
    "        return \"NEUTRAL\"\n",
    "    else:\n",
    "        return \"POSITIVE\"\n",
    "\n",
    "# creacion del grafico de pie de las reviews segun rating\n",
    "data[\"rating\"] = data[\"rating\"].apply(ratingTransform)\n",
    "plt.pie(data[\"rating\"].value_counts(), labels=data[\"rating\"].value_counts().index, autopct='%1.1f%%')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que la mayoría de las reseñas son positivas, luego vienen las negativas y por último las neutrales. Esto nos sugiere que la aplicación es buena, pero tiene algunos problemas que se pueden mejorar.\n",
    "\n",
    "Ya que nuestro dataset contiene en su mayoría texto, generaremos columnas que nos ayuden a analizarlo. Estas columnas serán:\n",
    "- Review Length: Cantidad de palabras que contiene el comentario.\n",
    "- Word Count: Cantidad de palabras que contiene el comentario.\n",
    "- Mean Word Length: Promedio de largo de las palabras del comentario.\n",
    "- Mean Sentence Length: Promedio de largo de las oraciones del comentario.\n",
    "\n",
    "Estas columnas nos ayudarán a realizar gráficos basados en atributos provenientes del texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tabla generada con el largo en caracteres de las reviews escritas\n",
    "data['Length'] = data['review_description'].str.len()\n",
    "\n",
    "# numero de palabras en la primera revie del dataset\n",
    "word_count = data['review_description'][0].split()\n",
    "\n",
    "# funcion para separar una oracion y contar la cantidad de palabras que posee\n",
    "def word_count(review):\n",
    "    review_list = review.split()\n",
    "    return len(review_list)\n",
    "\n",
    "# generacion de nueva columna de cantidad de palabras por review\n",
    "data['Word_count'] = data['review_description'].apply(word_count)\n",
    "\n",
    "# largo promedio de las palabras en cada review\n",
    "data['mean_word_length'] = data['review_description'].map(lambda rev: np.mean([len(word) for word in rev.split()]))\n",
    "\n",
    "# largo promedio de las oraciones en cada review\n",
    "np.mean([len(sent) for sent in tokenize.sent_tokenize(data['review_description'][0])])\n",
    "data['mean_sent_length'] = data['review_description'].map(lambda rev: np.mean([len(sent) for sent in tokenize.sent_tokenize(rev)]))\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creacion del boxplot de las palabras\n",
    "def visualize(col, ind=0, max_ys=[0.0085,0.047,0.18,0.02], max_xs=[400,100,15,200]):\n",
    "    \n",
    "    print()\n",
    "    plt.plot(1,2,1)\n",
    "    sns.boxplot(y=data[col], hue=data['rating'])\n",
    "    plt.title('Distribución de '+col)\n",
    "    plt.ylabel(col, labelpad=12.5)\n",
    "    plt.yscale('log') \n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(1,2,2)\n",
    "    sns.kdeplot(data=data, x=col, hue='rating')\n",
    "    plt.legend(data['rating'].unique())\n",
    "    plt.ylim([0,max_ys[ind]])\n",
    "    plt.xlim([0,max_xs[ind]])\n",
    "    plt.title('Estimación de densidad de '+col)\n",
    "    plt.xlabel(col, labelpad=12.5)\n",
    "    plt.ylabel('Frecuencia relativa')\n",
    "    \n",
    "    \n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "features = data.columns.tolist()[4:]\n",
    "print(features)\n",
    "i = 0\n",
    "for feature in features:\n",
    "    visualize(feature,i)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisis de frecuencia de términos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haremos uso de la librería NLTK, la cual nos permite realizar un análisis de frecuencia de términos. Para esto, primero debemos limpiar el texto, eliminando signos de puntuación, números, stopwords, etc. Esto lo hace gracias a que internamente NLTK posee una lista de stopwords en inglés, la cual usaremos para eliminarlas. Además, usaremos la librería string para eliminar signos de puntuación y números."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(review):\n",
    "    \n",
    "    review = review.lower()\n",
    "    review = re.sub('[^a-z A-Z 0-9-]+', '', review)\n",
    "    review = \" \".join([word for word in review.split() if word not in stopwords.words('english')])\n",
    "    \n",
    "    return review\n",
    "\n",
    "data['review_description'] = data['review_description'].apply(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pasamos las reviews a una lista de strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 palabras mas comunes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus(text):\n",
    "    text_list = text.split()\n",
    "    return text_list\n",
    "\n",
    "data['review_lists'] = data['review_description'].apply(corpus)\n",
    "\n",
    "corpus = []\n",
    "for i in trange(data.shape[0], ncols=150, nrows=10, colour='green', smoothing=0.8):\n",
    "    corpus += data['review_lists'][i]\n",
    "\n",
    "mostCommon = Counter(corpus).most_common(10)\n",
    "\n",
    "words = []\n",
    "freq = []\n",
    "for word, count in mostCommon:\n",
    "    words.append(word)\n",
    "    freq.append(count)\n",
    "\n",
    "sns.barplot(x=freq, y=words)\n",
    "plt.title('Top 10 Palabras más comunes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generacion de la wordcloud para definir cuales pueden ser las palabras mas importantes\n",
    "def plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), color = 'white',\n",
    "                   title = None, title_size=40, image_color=False):\n",
    "    stopwords = set(STOPWORDS)\n",
    "    more_stopwords = {'u', \"im\"}\n",
    "    stopwords = stopwords.union(more_stopwords)\n",
    "\n",
    "    wordcloud = WordCloud(background_color=color,\n",
    "                    stopwords = stopwords,\n",
    "                    max_words = max_words,\n",
    "                    max_font_size = max_font_size, \n",
    "                    random_state = 42,\n",
    "                    width=400, \n",
    "                    height=200,\n",
    "                    mask = mask)\n",
    "    wordcloud.generate(str(text))\n",
    "    \n",
    "    plt.figure(figsize=figure_size)\n",
    "    if image_color:\n",
    "        image_colors = ImageColorGenerator(mask);\n",
    "        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n",
    "        plt.title(title, fontdict={'size': title_size,  \n",
    "                                  'verticalalignment': 'bottom'})\n",
    "    else:\n",
    "        plt.imshow(wordcloud);\n",
    "        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n",
    "                                  'verticalalignment': 'bottom'})\n",
    "    plt.axis('off');\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Los $N-$ gramas mas frecuentes\n",
    "\n",
    "Los $N-$ gramas son secuencias de $N$ palabras consecutivas en un texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(2,2))\n",
    "bigrams = cv.fit_transform(data['review_description'])\n",
    "\n",
    "count_values = bigrams.toarray().sum(axis=0)\n",
    "ngram_freq = pd.DataFrame(sorted([(count_values[i], k) for k, i in cv.vocabulary_.items()], reverse = True))\n",
    "ngram_freq.columns = [\"frequency\", \"ngram\"]\n",
    "\n",
    "sns.barplot(x=ngram_freq['frequency'][:10], y=ngram_freq['ngram'][:10])\n",
    "plt.title('Top 10 2-gramas más comunes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv1 = CountVectorizer(ngram_range=(3,3))\n",
    "trigrams = cv1.fit_transform(data['review_description'])\n",
    "count_values = trigrams.toarray().sum(axis=0)\n",
    "ngram_freq = pd.DataFrame(sorted([(count_values[i], k) for k, i in cv1.vocabulary_.items()], reverse = True))\n",
    "ngram_freq.columns = [\"frequency\", \"ngram\"]\n",
    "\n",
    "sns.barplot(x=ngram_freq['frequency'][:10], y=ngram_freq['ngram'][:10])\n",
    "plt.title('Top 10 3-gramas más comunes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preguntas y Problemas\n",
    "\n",
    "1. ¿Existe una asociacion entre las personas que comparan threads con twiter y una puntuacion negativa?\\n\",\n",
    "2. ¿Es posible predecir si una reseña es positiva dado ciertos N-gramas?\"\n",
    "3. ¿Que palabras se asocian a una reseña positiva?\n",
    "4. ¿Cuales palabras se asocian a una reseña negativa?\n",
    "5. ¿Hay diferencias entre las reseñas positivas y negativas en palabras usadas, tamnaño de las reseñas, etc.?\n",
    "6. ¿Cómo se puede determinar el impacto de una actualización de una aplicación mediante las reseñas de usuarios?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
